{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef sigmoid_derivative(a):\n    return a * (1 - a)\n\n\n\n\nX = np.array([0,1,2,3,4,5])\ny = np.array([0,2,4,6,8,10])\n\n\nw1 = np.random.randn()   # input → hidden\nb1 = np.random.randn()\n\nw2 = np.random.randn()   # hidden → output\nb2 = np.random.randn()\n\nlr = 0.01   # smaller LR works better for linear output\nepochs = 10000\n\n\n\nfor epoch in range(epochs):\n\n    # Forward Propagation\n    z1 = w1 * X + b1\n    a1 = sigmoid(z1)        # hidden layer activation\n\n    z2 = w2 * a1 + b2\n    y_pred = z2             #  LINEAR OUTPUT\n\n\n    #  Loss (MSE)\n    loss = np.mean((y - y_pred) ** 2)\n\n\n    # Backpropagation\n    error = y_pred - y\n\n    # Output layer derivative = 1 (because linear)\n    dz2 = error\n\n    dw2 = np.mean(dz2 * a1)\n    db2 = np.mean(dz2)\n\n    dz1 = dz2 * w2 * sigmoid_derivative(a1)\n\n    dw1 = np.mean(dz1 * X)\n    db1 = np.mean(dz1)\n\n\n    # Update\n    w2 -= lr * dw2\n    b2 -= lr * db2\n\n    w1 -= lr * dw1\n    b1 -= lr * db1\n\n\n    if epoch % 1000 == 0:\n        print(\"Epoch:\", epoch, \"Loss:\", round(loss,4))\n\n\n\nprint(\"\\nTraining done!\")\n\nprint(\"w1:\", round(w1,2), \"b1:\", round(b1,2))\nprint(\"w2:\", round(w2,2), \"b2:\", round(b2,2))\n\n\n# Testing\n\ntest_x = 7\n\nz1 = w1 * test_x + b1\na1 = sigmoid(z1)\n\nprediction = w2 * a1 + b2   # linear output\n\nprint(\"\\nInput:\", test_x)\nprint(\"Prediction:\", round(prediction,2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T16:57:26.622626Z","iopub.execute_input":"2026-02-06T16:57:26.624290Z","iopub.status.idle":"2026-02-06T16:57:27.153387Z","shell.execute_reply.started":"2026-02-06T16:57:26.624240Z","shell.execute_reply":"2026-02-06T16:57:27.152266Z"}},"outputs":[{"name":"stdout","text":"Epoch: 0 Loss: 26.5724\nEpoch: 1000 Loss: 0.9825\nEpoch: 2000 Loss: 0.4677\nEpoch: 3000 Loss: 0.3521\nEpoch: 4000 Loss: 0.2805\nEpoch: 5000 Loss: 0.2324\nEpoch: 6000 Loss: 0.1986\nEpoch: 7000 Loss: 0.1738\nEpoch: 8000 Loss: 0.155\nEpoch: 9000 Loss: 0.1402\n\nTraining done!\nw1: 0.92 b1: -2.35\nw2: 11.09 b2: -0.43\n\nInput: 7\nPrediction: 10.47\n","output_type":"stream"}],"execution_count":6}]}